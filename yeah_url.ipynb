{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPJdroCudcofJqRjI3l4y1b",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hk-vk/CursorGroqProxy/blob/main/yeah_url.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Install necassary modules**"
      ],
      "metadata": {
        "id": "QuOBnH4AeSH6"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EhCG7oSQbqIv",
        "outputId": "81243db2-896d-44d3-c17d-beaf932ae600"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (2.2.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (1.26.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (2.32.3)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (4.12.3)\n",
            "Requirement already satisfied: tldextract in /usr/local/lib/python3.10/dist-packages (5.1.3)\n",
            "Requirement already satisfied: python-whois in /usr/local/lib/python3.10/dist-packages (0.9.5)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (1.6.0)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.47.1)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.5.1+cu121)\n",
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.10/dist-packages (2.17.1)\n",
            "Requirement already satisfied: nanoid in /usr/local/lib/python3.10/dist-packages (2.0.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests) (2024.12.14)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4) (2.6)\n",
            "Requirement already satisfied: requests-file>=1.4 in /usr/local/lib/python3.10/dist-packages (from tldextract) (2.1.0)\n",
            "Requirement already satisfied: filelock>=3.0.8 in /usr/local/lib/python3.10/dist-packages (from tldextract) (3.16.1)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.13.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (3.5.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.24.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.27.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.21.0)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2024.10.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (24.3.25)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: h5py>=3.10.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.12.1)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (18.1.1)\n",
            "Requirement already satisfied: ml-dtypes<0.5.0,>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.4.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.4.0)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (4.25.5)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow) (75.1.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.17.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.5.0)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.17.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.68.1)\n",
            "Requirement already satisfied: tensorboard<2.18,>=2.17 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.17.1)\n",
            "Requirement already satisfied: keras>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.5.0)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.37.1)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow) (0.45.1)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->tensorflow) (13.9.4)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->tensorflow) (0.0.8)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->tensorflow) (0.13.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.18,>=2.17->tensorflow) (3.7)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.18,>=2.17->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.18,>=2.17->tensorflow) (3.1.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras>=3.2.0->tensorflow) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras>=3.2.0->tensorflow) (2.18.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.2.0->tensorflow) (0.1.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install pandas numpy requests beautifulsoup4 tldextract python-whois scikit-learn transformers torch tensorflow nanoid\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import requests\n",
        "import tldextract\n",
        "import whois\n",
        "from datetime import datetime\n",
        "from bs4 import BeautifulSoup\n",
        "from urllib.parse import urlparse\n",
        "import re\n",
        "from transformers import pipeline\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "import tensorflow as tf\n",
        "import nanoid\n"
      ],
      "metadata": {
        "id": "Lss2TJJkeh2b"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_trusted_sources_db():\n",
        "    return {\n",
        "        # Major News Sources\n",
        "        'manoramaonline.com': {'name': 'Malayala Manorama', 'credibility_score': 0.95, 'established': 1888},\n",
        "        'mathrubhumi.com': {'name': 'Mathrubhumi', 'credibility_score': 0.95, 'established': 1923},\n",
        "        'madhyamam.com': {'name': 'Madhyamam', 'credibility_score': 0.90, 'established': 1987},\n",
        "        'deshabhimani.com': {'name': 'Deshabhimani', 'credibility_score': 0.85, 'established': 1942},\n",
        "        'keralakaumudi.com': {'name': 'Kerala Kaumudi', 'credibility_score': 0.90, 'established': 1911},\n",
        "\n",
        "        # Regional Sources\n",
        "        'chandrikadaily.com': {'name': 'Chandrika', 'credibility_score': 0.80, 'region': 'Kozhikode'},\n",
        "        'janmabhumi.in': {'name': 'Janmabhumi', 'credibility_score': 0.80, 'region': 'Thiruvananthapuram'},\n",
        "        'sirajlive.com': {'name': 'Siraj Daily', 'credibility_score': 0.75, 'region': 'Kozhikode'},\n",
        "        'metrovaartha.com': {'name': 'Metro Vaartha', 'credibility_score': 0.75, 'region': 'Kochi'},\n",
        "\n",
        "        # Online News Portals\n",
        "        'southlive.in': {'name': 'South Live', 'credibility_score': 0.70},\n",
        "        'thejasnews.com': {'name': 'Thejas News', 'credibility_score': 0.70},\n",
        "        '24newslive.com': {'name': '24 News', 'credibility_score': 0.75},\n",
        "\n",
        "        # Fact Checkers\n",
        "        'malayalam.factcrescendo.com': {'name': 'Fact Crescendo Malayalam', 'credibility_score': 0.90},\n",
        "        'malayalam.vishvasnews.com': {'name': 'Vishvas News Malayalam', 'credibility_score': 0.85}\n",
        "    }\n"
      ],
      "metadata": {
        "id": "w7EJbe1kb-t3"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class URLFeatureExtractor:\n",
        "    def __init__(self):\n",
        "        self.trusted_sources = create_trusted_sources_db()\n",
        "        self.suspicious_tlds = ['xyz', 'top', 'buzz', 'guru', 'club', 'online']\n",
        "\n",
        "    def extract_features(self, url):\n",
        "        try:\n",
        "            parsed_url = urlparse(url)\n",
        "            domain = tldextract.extract(url).registered_domain\n",
        "\n",
        "            features = {\n",
        "                'url_length': len(url),\n",
        "                'domain_length': len(domain),\n",
        "                'path_length': len(parsed_url.path),\n",
        "                'num_dots': url.count('.'),\n",
        "                'num_hyphens': url.count('-'),\n",
        "                'num_underscores': url.count('_'),\n",
        "                'num_slashes': url.count('/'),\n",
        "                'num_equals': url.count('='),\n",
        "                'num_digits': sum(c.isdigit() for c in url),\n",
        "                'has_https': int(url.startswith('https://')),\n",
        "                'is_trusted_domain': int(domain in self.trusted_sources),\n",
        "                'has_suspicious_tld': int(tldextract.extract(url).suffix in self.suspicious_tlds)\n",
        "            }\n",
        "\n",
        "            # Add domain age\n",
        "            try:\n",
        "                domain_info = whois.whois(domain)\n",
        "                creation_date = domain_info.creation_date\n",
        "                if isinstance(creation_date, list):\n",
        "                    creation_date = creation_date[0]\n",
        "                features['domain_age'] = (datetime.now() - creation_date).days\n",
        "            except:\n",
        "                features['domain_age'] = -1\n",
        "\n",
        "            return features\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error extracting features: {str(e)}\")\n",
        "            return None\n"
      ],
      "metadata": {
        "id": "m7vPLIpmcLUu"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class URLCredibilityAnalyzer:\n",
        "    def __init__(self):\n",
        "        self.feature_extractor = URLFeatureExtractor()\n",
        "        self.model = RandomForestClassifier()\n",
        "\n",
        "    def check_ssl_certificate(self, url):\n",
        "        try:\n",
        "            response = requests.get(url, verify=True, timeout=10)\n",
        "            return True\n",
        "        except:\n",
        "            return False\n",
        "\n",
        "    def analyze_url(self, url):\n",
        "        try:\n",
        "            domain = tldextract.extract(url).registered_domain\n",
        "            features = self.feature_extractor.extract_features(url)\n",
        "\n",
        "            if features is None:\n",
        "                return {\n",
        "                    'credibility_score': 0.0,\n",
        "                    'status': 'error',\n",
        "                    'message': 'Failed to extract features'\n",
        "                }\n",
        "\n",
        "            # Check trusted sources first\n",
        "            if domain in self.feature_extractor.trusted_sources:\n",
        "                source_info = self.feature_extractor.trusted_sources[domain]\n",
        "                return {\n",
        "                    'credibility_score': source_info['credibility_score'],\n",
        "                    'status': 'trusted',\n",
        "                    'source_name': source_info['name'],\n",
        "                    'features': features\n",
        "                }\n",
        "\n",
        "            # Calculate credibility score\n",
        "            score = 0.5  # Base score\n",
        "\n",
        "            if features['has_https']:\n",
        "                score += 0.1\n",
        "            if features['domain_age'] > 365:\n",
        "                score += 0.1\n",
        "            if features['has_suspicious_tld']:\n",
        "                score -= 0.2\n",
        "\n",
        "            return {\n",
        "                'credibility_score': max(min(score, 1.0), 0.0),\n",
        "                'status': 'analyzed',\n",
        "                'features': features,\n",
        "                'warning_flags': self._get_warning_flags(features)\n",
        "            }\n",
        "\n",
        "        except Exception as e:\n",
        "            return {\n",
        "                'credibility_score': 0.0,\n",
        "                'status': 'error',\n",
        "                'message': str(e)\n",
        "            }\n",
        "\n",
        "    def _get_warning_flags(self, features):\n",
        "        flags = []\n",
        "        if not features['has_https']:\n",
        "            flags.append('No HTTPS security')\n",
        "        if features['domain_age'] < 180:\n",
        "            flags.append('Recently registered domain')\n",
        "        if features['has_suspicious_tld']:\n",
        "            flags.append('Suspicious top-level domain')\n",
        "        return flags\n"
      ],
      "metadata": {
        "id": "EnqgO3vMcTi3"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class URLExistenceChecker:\n",
        "    def __init__(self):\n",
        "        self.session = requests.Session()\n",
        "        self.timeout = 10\n",
        "        self.headers = {\n",
        "            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'\n",
        "        }\n",
        "\n",
        "    def check_url_exists(self, url):\n",
        "        try:\n",
        "            # Add scheme if not present\n",
        "            if not url.startswith(('http://', 'https://')):\n",
        "                url = 'https://' + url\n",
        "\n",
        "            # Try HEAD request first (faster)\n",
        "            response = self.session.head(\n",
        "                url,\n",
        "                timeout=self.timeout,\n",
        "                allow_redirects=True,\n",
        "                headers=self.headers\n",
        "            )\n",
        "\n",
        "            # If HEAD fails, try GET\n",
        "            if response.status_code != 200:\n",
        "                response = self.session.get(\n",
        "                    url,\n",
        "                    timeout=self.timeout,\n",
        "                    allow_redirects=True,\n",
        "                    headers=self.headers\n",
        "                )\n",
        "\n",
        "            return {\n",
        "                'exists': True,\n",
        "                'status_code': response.status_code,\n",
        "                'accessible': response.status_code == 200,\n",
        "                'final_url': response.url,\n",
        "                'is_redirect': len(response.history) > 0,\n",
        "                'content_type': response.headers.get('content-type', '')\n",
        "            }\n",
        "\n",
        "        except requests.ConnectionError:\n",
        "            return {\n",
        "                'exists': False,\n",
        "                'error': 'Connection failed',\n",
        "                'reason': 'Unable to connect to server'\n",
        "            }\n",
        "        except requests.Timeout:\n",
        "            return {\n",
        "                'exists': False,\n",
        "                'error': 'Timeout',\n",
        "                'reason': 'Request timed out'\n",
        "            }\n",
        "        except requests.RequestException as e:\n",
        "            return {\n",
        "                'exists': False,\n",
        "                'error': 'Request failed',\n",
        "                'reason': str(e)\n",
        "            }\n"
      ],
      "metadata": {
        "id": "B64YVLa-nF6_"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def verify_url(url):\n",
        "    # First check if URL exists and is accessible\n",
        "    existence_checker = URLExistenceChecker()\n",
        "    existence_result = existence_checker.check_url_exists(url)\n",
        "\n",
        "    print(f\"\\nURL Verification Results for: {url}\")\n",
        "    print(\"-\" * 50)\n",
        "\n",
        "    # If URL doesn't exist or isn't accessible, return early\n",
        "    if not existence_result['exists']:\n",
        "        print(f\"❌ URL Not Accessible\")\n",
        "        print(f\"Error: {existence_result['error']}\")\n",
        "        print(f\"Reason: {existence_result['reason']}\")\n",
        "        return existence_result\n",
        "\n",
        "    # If URL exists, proceed with credibility analysis\n",
        "    analyzer = URLCredibilityAnalyzer()\n",
        "    result = analyzer.analyze_url(url)\n",
        "\n",
        "    # Print existence details\n",
        "    print(f\"URL Status: {'✅ Accessible' if existence_result['accessible'] else '❌ Not Accessible'}\")\n",
        "    print(f\"Status Code: {existence_result['status_code']}\")\n",
        "    if existence_result['is_redirect']:\n",
        "        print(f\"Redirected to: {existence_result['final_url']}\")\n",
        "\n",
        "    print(\"\\nCredibility Analysis:\")\n",
        "    if result['status'] == 'trusted':\n",
        "        print(f\"✅ Trusted Source: {result['source_name']}\")\n",
        "        print(f\"Credibility Score: {result['credibility_score']:.2f}\")\n",
        "\n",
        "    elif result['status'] == 'analyzed':\n",
        "        print(f\"Credibility Score: {result['credibility_score']:.2f}\")\n",
        "        if result['warning_flags']:\n",
        "            print(\"\\nWarning Flags:\")\n",
        "            for flag in result['warning_flags']:\n",
        "                print(f\"⚠️ {flag}\")\n",
        "\n",
        "    else:\n",
        "        print(f\"❌ Error: {result['message']}\")\n",
        "\n",
        "    return {\n",
        "        'existence_check': existence_result,\n",
        "        'credibility_check': result,\n",
        "        'final_score': result.get('credibility_score', 0.0),\n",
        "        'is_accessible': existence_result['accessible'],\n",
        "        'is_trusted': result['status'] == 'trusted'\n",
        "    }\n",
        "test_urls = [\n",
        "    \"https://www.manoramaonline.com/news/latest-news.html\",\n",
        "     \"https://www.sirajlive.com/plane-crash-in-south-korea-28-dead.html\",\n",
        "     \"https://thisnotexit.daily/\",\n",
        "     \"https://www.news.xyz\"\n",
        "]\n",
        "\n",
        "for url in test_urls:\n",
        "    verify_url(url)\n",
        "    print(\"\\n\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zTbhEZwlce_E",
        "outputId": "596aeca2-89ed-4b56-8066-891e66cd6682"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "URL Verification Results for: https://www.manoramaonline.com/news/latest-news.html\n",
            "--------------------------------------------------\n",
            "URL Status: ✅ Accessible\n",
            "Status Code: 200\n",
            "\n",
            "Credibility Analysis:\n",
            "✅ Trusted Source: Malayala Manorama\n",
            "Credibility Score: 0.95\n",
            "\n",
            "\n",
            "\n",
            "URL Verification Results for: https://www.sirajlive.com/plane-crash-in-south-korea-28-dead.html\n",
            "--------------------------------------------------\n",
            "URL Status: ✅ Accessible\n",
            "Status Code: 200\n",
            "\n",
            "Credibility Analysis:\n",
            "✅ Trusted Source: Siraj Daily\n",
            "Credibility Score: 0.75\n",
            "\n",
            "\n",
            "\n",
            "URL Verification Results for: https://thisnotexit.daily/\n",
            "--------------------------------------------------\n",
            "❌ URL Not Accessible\n",
            "Error: Connection failed\n",
            "Reason: Unable to connect to server\n",
            "\n",
            "\n",
            "\n",
            "URL Verification Results for: https://www.news.xyz\n",
            "--------------------------------------------------\n",
            "❌ URL Not Accessible\n",
            "Error: Connection failed\n",
            "Reason: Unable to connect to server\n",
            "\n",
            "\n"
          ]
        }
      ]
    }
  ]
}